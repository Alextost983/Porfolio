{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  ---\n",
    " # Структура проекта\n",
    " ### 1.Выгрузка данных\n",
    " -  Ознакомление и предобработка данных из таблицы с категориями фильмов\n",
    " -  Выгрузка субтитров и сопоставление с названиями фильмов\n",
    " -  Очистка от дубликатов\n",
    " -  Создание сводной таблицы с данными\n",
    " -  Выгрузка словаря \n",
    " ### 2.Фильтрация слов и разбиение на ключевые блоки\n",
    " -  Лемматизация и добавление новых фитч на основе словаря оксфорда\n",
    " -  Исключение стоп сов из данных\n",
    " ### 3.Препроцессинг\n",
    " -  Разбиение данных на выборки\n",
    " -  Ознакомление с дисбалансом\n",
    " ### 4. Обучение моделей\n",
    " ### 5. Вывод  \n",
    " --- \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Выгрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.1.2)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from openpyxl) (1.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pysrt in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.1.2)\n",
      "Requirement already satisfied: chardet in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pysrt) (5.1.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.6.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (1.10.11)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\program files\\windowsapps\\pythonsoftwarefoundation.python.3.11_3.11.1264.0_x64__qbz5n2kfra8p0\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (23.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.6.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->spacy) (2.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: sklearn in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (0.0.post7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from scikit-learn) (3.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nltk in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (1.3.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: reportlab in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (4.0.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from reportlab) (9.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip  install openpyxl\n",
    "%pip install pysrt\n",
    "%pip install spacy\n",
    "%pip install sklearn\n",
    "%pip install scikit-learn\n",
    "%pip install nltk\n",
    "%pip install reportlab\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyPDF2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.0.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: pysrt in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.1.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (3.8.1)\n",
      "Requirement already satisfied: pyprind in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.11.3)\n",
      "Requirement already satisfied: joblib in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: chardet in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pysrt) (5.1.0)\n",
      "Requirement already satisfied: click in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (8.1.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import io\n",
    "!{sys.executable} -m pip install pandas pysrt nltk pyprind joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import string\n",
    "import spacy \n",
    "import pysrt\n",
    "import re\n",
    "from time import time\n",
    "from joblib import dump, load\n",
    "import pyprind\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import PyPDF2\n",
    "from reportlab.pdfgen import canvas\n",
    "from openpyxl import Workbook\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_files \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.extmath import density\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.pdfinterp import PDFPageInterpreter\n",
    "from pdfminer.pdfinterp import PDFResourceManager\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from PyPDF2 import PdfReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Ознакомление и предобработка данных из таблицы с категориями фильмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>B1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>A2/A2+</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                             Movie   Level\n",
       "0   0         10_Cloverfield_lane(2016)      B1\n",
       "1   1  10_things_I_hate_about_you(1999)      B1\n",
       "2   2              A_knights_tale(2001)      B2\n",
       "3   3              A_star_is_born(2018)      B2\n",
       "4   4                     Aladdin(1992)  A2/A2+"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataeng='movies_labels.xlsx'\n",
    "data = pd.read_excel(dataeng)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, Movie, Level]\n",
       "Index: []"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.duplicated(keep=False)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим уникальность значений и проведем перекодировку признаков, назначив индексы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['B1', 'B2', 'A2/A2+', 'C1', 'B1, B2', 'A2/A2+, B1', 'A2'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Level'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Заменим имеющиеся категории на соответствующие, объеденив некоторые."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_level = {\n",
    "  'B1': 2,\n",
    "  'B2': 3, \n",
    "  'A2/A2+': 1,\n",
    "  'C1': 4,\n",
    "  'B1, B2': 3, \n",
    "  'A2/A2+, B1': 2, \n",
    "  'A2': 1\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                             Movie  Level\n",
       "0   0         10_Cloverfield_lane(2016)      2\n",
       "1   1  10_things_I_hate_about_you(1999)      2\n",
       "2   2              A_knights_tale(2001)      3\n",
       "3   3              A_star_is_born(2018)      3\n",
       "4   4                     Aladdin(1992)      1"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data.replace(new_level)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Movie</th>\n",
       "      <th>Level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, Movie, Level]\n",
       "Index: []"
      ]
     },
     "execution_count": 456,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.duplicated(keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дубликатов не обнаружено"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    " -  Выгрузка словаря \n",
    " \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим файл с колонками соответствующими уровню английского"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем Pdf файл со словарем и трансформируем в xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Открываем PDF файл\n",
    "pdf_file = open('\\\\Users\\Asus\\Desktop\\All_project\\Category_of_english_Level\\Oxford_CEFR_level\\The_Oxford_5000_by_CEFR_level.pdf', 'rb')\n",
    "pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
    "\n",
    "# Создаем новый файл Excel\n",
    "wb = Workbook()\n",
    "ws = wb.active\n",
    "\n",
    "# Извлекаем текст из каждой страницы PDF файла\n",
    "for page in pdf_reader.pages:\n",
    "    text = page.extract_text()\n",
    "    \n",
    "    # Записываем текст в ячейку Excel\n",
    "    ws.append([text])\n",
    "\n",
    "# Сохраняем файл Excel\n",
    "wb.save('output.xlsx')\n",
    "\n",
    "# Закрываем PDF файл\n",
    "pdf_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осущетвили ручной перенос слов, разделяя их по категориям"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Взглянем что получилось"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A2</th>\n",
       "      <th>B1</th>\n",
       "      <th>B2</th>\n",
       "      <th>C1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ability</td>\n",
       "      <td>absolutely academic</td>\n",
       "      <td>abandon</td>\n",
       "      <td>abolish</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>able</td>\n",
       "      <td>access</td>\n",
       "      <td>absolute</td>\n",
       "      <td>abortion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abroad</td>\n",
       "      <td>accommodation</td>\n",
       "      <td>academic</td>\n",
       "      <td>absence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>accept</td>\n",
       "      <td>account</td>\n",
       "      <td>acceptable</td>\n",
       "      <td>absent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>accident</td>\n",
       "      <td>achievement</td>\n",
       "      <td>accompany</td>\n",
       "      <td>absurd</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A2                    B1           B2         C1\n",
       "0   ability   absolutely academic      abandon    abolish \n",
       "1      able                access     absolute   abortion \n",
       "2    abroad         accommodation     academic    absence \n",
       "3    accept               account   acceptable     absent \n",
       "4  accident           achievement    accompany     absurd "
      ]
     },
     "execution_count": 459,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = pd.read_excel('\\\\Users\\Asus\\Desktop\\All_project\\Category_of_english_Level\\English_level_env\\words_category.xlsx')\n",
    "vocab.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные требуют чистки, поэтому удалим лишние символы и слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab.astype(str)\n",
    "# Создадим функцию для очистки элементов колонки\n",
    "def clean_column_value(value):\n",
    "    # Удалите лишние символы и слова с помощью регулярных выражений\n",
    "    cleaned_value = re.sub(r'\\W+', ' ', value)  # удаление не алфавитных символов\n",
    "    cleaned_value = re.sub(r'\\b\\w{1,2}\\b', '', cleaned_value)  # удаление слов меньше 3 символов   \n",
    "    stop_words_vocab = ['adv','pron', 'det', 'adj', 'prep', 'modal', 'exclam']\n",
    "\n",
    "    # Формируем шаблон для всех стоп-слов в виде '|word1|word2|word3|...'\n",
    "    pattern = '|'.join(stop_words_vocab)\n",
    "\n",
    "    # Заменяем все совпадения со стоп-словами на пустую строку\n",
    "    cleaned_value = re.sub(pattern, '', cleaned_value)\n",
    "\n",
    "    return cleaned_value \n",
    "    # Применим функцию ко всем колонкам датасета #for column in vocab: \n",
    "\n",
    "vocab['A2'] = vocab['A2'].apply(clean_column_value)\n",
    "vocab['B1'] = vocab['B1'].apply(clean_column_value)\n",
    "vocab['B2'] = vocab['B2'].apply(clean_column_value)\n",
    "vocab['C1'] = vocab['C1'].apply(clean_column_value)\n",
    "    # Выведите очищенную колонку на экран\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A2                    B1           B2         C1\n",
      "0   ability   absolutely academic      abandon    abolish \n",
      "1      able                access     absolute   abortion \n",
      "2    abroad         accommodation     academic    absence \n",
      "3    accept               account   acceptable     absent \n",
      "4  accident           achievement    accompany     absurd \n"
     ]
    }
   ],
   "source": [
    "print(vocab.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vocab.fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим дополнительные словари под каждый уровень"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['abolish ',\n",
       " 'abortion ',\n",
       " 'absence ',\n",
       " 'absent ',\n",
       " 'absurd ',\n",
       " 'abundance ',\n",
       " 'abuse ',\n",
       " 'academy ',\n",
       " 'accelerate ',\n",
       " 'acceptance ',\n",
       " 'accessible ',\n",
       " 'accomplishment ',\n",
       " 'accordance ',\n",
       " 'accordingly ',\n",
       " 'accountability ',\n",
       " 'accountable ',\n",
       " 'accumulate ',\n",
       " 'accumulation ',\n",
       " 'accusation ',\n",
       " 'accused ',\n",
       " 'acid ',\n",
       " 'acquisition ',\n",
       " 'acre ',\n",
       " 'activation activist ',\n",
       " 'acute ',\n",
       " 'adaptation ',\n",
       " 'adhere ',\n",
       " 'acent ',\n",
       " 'ustment ',\n",
       " 'administer ',\n",
       " 'administrative ',\n",
       " 'administrator ',\n",
       " 'admission ',\n",
       " 'adolescent ',\n",
       " 'adoption ',\n",
       " 'erse ',\n",
       " 'ocate ',\n",
       " 'aesthetic ',\n",
       " 'affection ',\n",
       " 'aftermath ',\n",
       " 'aggression ',\n",
       " 'agricultural ',\n",
       " 'aide ',\n",
       " 'albeit conj ',\n",
       " 'alert ',\n",
       " 'alien ',\n",
       " 'align ',\n",
       " 'alignment ',\n",
       " 'alike ',\n",
       " 'allegation ',\n",
       " 'allege ',\n",
       " 'allegedly ',\n",
       " 'alliance ',\n",
       " 'allocate ',\n",
       " 'allocation ',\n",
       " 'allowance ',\n",
       " 'ally ',\n",
       " 'aluminium ',\n",
       " 'amateur ',\n",
       " 'ambassador ',\n",
       " 'amend ',\n",
       " 'amendment ',\n",
       " 'amid ',\n",
       " 'analogy ',\n",
       " 'anchor ',\n",
       " 'angel ',\n",
       " 'anonymous ',\n",
       " 'apparatus ',\n",
       " 'appealing ',\n",
       " 'appetite ',\n",
       " 'applaud ',\n",
       " 'applicable ',\n",
       " 'appoint ',\n",
       " 'appreciation ',\n",
       " 'arbitrary ',\n",
       " 'architectural ',\n",
       " 'archive ',\n",
       " 'arena ',\n",
       " 'arguably ',\n",
       " 'arm ',\n",
       " 'array ',\n",
       " 'articulate ',\n",
       " 'ash ',\n",
       " 'aspiration ',\n",
       " 'aspire ',\n",
       " 'assassination ',\n",
       " 'assault ',\n",
       " 'assemble ',\n",
       " 'assembly ',\n",
       " 'assert ',\n",
       " 'assertion ',\n",
       " 'assurance ',\n",
       " 'asylum ',\n",
       " 'attain ',\n",
       " 'attendance ',\n",
       " 'attorney ',\n",
       " 'attribute ',\n",
       " 'audit ',\n",
       " 'authentic ',\n",
       " 'authorize ',\n",
       " 'auto ',\n",
       " 'autonomy ',\n",
       " 'availability ',\n",
       " 'await ',\n",
       " 'backdrop ',\n",
       " 'backing ',\n",
       " 'backup ',\n",
       " 'bail ',\n",
       " 'ballot ',\n",
       " 'banner ',\n",
       " 'bare ',\n",
       " 'barrel ',\n",
       " 'bass ',\n",
       " 'bat ',\n",
       " 'battlefield ',\n",
       " 'bay ',\n",
       " 'beam ',\n",
       " 'beast ',\n",
       " 'behalf ',\n",
       " 'beloved ',\n",
       " 'bench ',\n",
       " 'benchmark ',\n",
       " 'beneath ',\n",
       " 'beneficiary ',\n",
       " 'betray ',\n",
       " 'bind ',\n",
       " 'biography ',\n",
       " 'bishop ',\n",
       " 'bizarre ',\n",
       " 'blade ',\n",
       " 'blast ',\n",
       " 'bleed ',\n",
       " 'blend ',\n",
       " 'bless ',\n",
       " 'blessing ',\n",
       " 'boast ',\n",
       " 'bonus ',\n",
       " 'boom ',\n",
       " 'bounce ',\n",
       " 'boundary ',\n",
       " 'bow ',\n",
       " 'breach ',\n",
       " 'breakdown ',\n",
       " 'breakthrough ',\n",
       " 'breed ',\n",
       " 'broadband ',\n",
       " 'browser ',\n",
       " 'brutal ',\n",
       " 'buck ',\n",
       " 'buddy ',\n",
       " 'buffer ',\n",
       " 'bulk ',\n",
       " 'burden ',\n",
       " 'bureaucracy ',\n",
       " 'burial ',\n",
       " 'burst ',\n",
       " 'cabinet ',\n",
       " 'calculation ',\n",
       " 'canvas ',\n",
       " 'capability ',\n",
       " 'capitalism ',\n",
       " 'capitalist cargo ',\n",
       " 'carriage ',\n",
       " 'carve ',\n",
       " 'casino ',\n",
       " 'casualty ',\n",
       " 'catalogue ',\n",
       " 'cater ',\n",
       " 'cattle ',\n",
       " 'caution ',\n",
       " 'cautious ',\n",
       " 'cease ',\n",
       " 'cemetery ',\n",
       " 'chamber ',\n",
       " 'chaos ',\n",
       " 'characterize ',\n",
       " 'charm ',\n",
       " 'charter ',\n",
       " 'chronic ',\n",
       " 'chunk ',\n",
       " 'circulate ',\n",
       " 'circulation ',\n",
       " 'citizenship ',\n",
       " 'civic ',\n",
       " 'civilian ',\n",
       " 'clarity ',\n",
       " 'clash ',\n",
       " 'classification ',\n",
       " 'cling ',\n",
       " 'clinical ',\n",
       " 'closure ',\n",
       " 'cluster ',\n",
       " 'coalition ',\n",
       " 'coastal ',\n",
       " 'cocktail ',\n",
       " 'cognitive ',\n",
       " 'coincide ',\n",
       " 'collaborate ',\n",
       " 'collaboration ',\n",
       " 'collective ',\n",
       " 'collision ',\n",
       " 'colonial ',\n",
       " 'columnist ',\n",
       " 'combat ',\n",
       " 'commence ',\n",
       " 'commentary ',\n",
       " 'commentator ',\n",
       " 'commerce ',\n",
       " 'commissioner ',\n",
       " 'commodity ',\n",
       " 'communist ',\n",
       " 'companion ',\n",
       " 'comparable ',\n",
       " 'compassion ',\n",
       " 'compel ',\n",
       " 'compelling ',\n",
       " 'compensate ',\n",
       " 'compensation ',\n",
       " 'competence ',\n",
       " 'competent ',\n",
       " 'compile ',\n",
       " 'complement ',\n",
       " 'complexity ',\n",
       " 'compliance ',\n",
       " 'complication ',\n",
       " 'comply ',\n",
       " 'composition ',\n",
       " 'compromise ',\n",
       " 'compute ',\n",
       " 'conceal ',\n",
       " 'concede conceive ',\n",
       " 'conception ',\n",
       " 'concession ',\n",
       " 'condemn ',\n",
       " 'confer ',\n",
       " 'confession ',\n",
       " 'configuration ',\n",
       " 'confine ',\n",
       " 'confirmation ',\n",
       " 'confront ',\n",
       " 'confrontation ',\n",
       " 'congratulate ',\n",
       " 'congregation ',\n",
       " 'congressional ',\n",
       " 'conquer ',\n",
       " 'conscience ',\n",
       " 'consciousness ',\n",
       " 'consecutive ',\n",
       " 'consensus ',\n",
       " 'consent ',\n",
       " 'conserve ',\n",
       " 'consistency ',\n",
       " 'consolidate ',\n",
       " 'constituency ',\n",
       " 'constitute ',\n",
       " 'constitution ',\n",
       " 'constitutional ',\n",
       " 'constraint ',\n",
       " 'consultation ',\n",
       " 'contemplate ',\n",
       " 'contempt ',\n",
       " 'contend ',\n",
       " 'contender ',\n",
       " 'content ',\n",
       " 'contention ',\n",
       " 'continually ',\n",
       " 'contractor ',\n",
       " 'contradiction ',\n",
       " 'contrary ',\n",
       " 'contributor ',\n",
       " 'conversion ',\n",
       " 'convict ',\n",
       " 'conviction ',\n",
       " 'cooperate ',\n",
       " 'cooperative ',\n",
       " 'coordinate ',\n",
       " 'coordination ',\n",
       " 'coordinator ',\n",
       " 'cop ',\n",
       " 'copper ',\n",
       " 'copyright ',\n",
       " 'correction ',\n",
       " 'correlate ',\n",
       " 'correlation ',\n",
       " 'correspond ',\n",
       " 'correspondence ',\n",
       " 'correspondent ',\n",
       " 'corresponding ',\n",
       " 'corrupt ',\n",
       " 'corruption ',\n",
       " 'costly ',\n",
       " 'councillor ',\n",
       " 'counselling ',\n",
       " 'counsellor ',\n",
       " 'counter argue against ',\n",
       " 'counterpart ',\n",
       " 'countless ',\n",
       " 'coup ',\n",
       " 'courtesy ',\n",
       " 'craft crawl ',\n",
       " 'creator ',\n",
       " 'credibility ',\n",
       " 'credible ',\n",
       " 'creep ',\n",
       " 'critique ',\n",
       " 'crown ',\n",
       " 'crude ',\n",
       " 'crush ',\n",
       " 'crystal ',\n",
       " 'cult ',\n",
       " 'cultivate ',\n",
       " 'curiosity ',\n",
       " 'custody ',\n",
       " 'cutting ',\n",
       " 'cynical ',\n",
       " 'dam ',\n",
       " 'damaging ',\n",
       " 'dawn ',\n",
       " 'debris ',\n",
       " 'debut ',\n",
       " 'decision making ',\n",
       " 'decisive ',\n",
       " 'declaration ',\n",
       " 'dedicated ',\n",
       " 'dedication ',\n",
       " 'deed ',\n",
       " 'deem ',\n",
       " 'default ',\n",
       " 'defect ',\n",
       " 'defensive ',\n",
       " 'deficiency ',\n",
       " 'deficit ',\n",
       " 'defy ',\n",
       " 'delegate ',\n",
       " 'delegation ',\n",
       " 'delicate ',\n",
       " 'demon ',\n",
       " 'denial ',\n",
       " 'denounce ',\n",
       " 'dense ',\n",
       " 'density ',\n",
       " 'dependence ',\n",
       " 'depict ',\n",
       " 'deploy ',\n",
       " 'deployment ',\n",
       " 'deposit ',\n",
       " 'deprive ',\n",
       " 'deputy ',\n",
       " 'descend ',\n",
       " 'descent ',\n",
       " 'designate ',\n",
       " 'desirable ',\n",
       " 'desktop ',\n",
       " 'destructive ',\n",
       " 'ain ',\n",
       " 'ection ',\n",
       " 'ention ',\n",
       " 'eriorate ',\n",
       " 'devastate ',\n",
       " 'devil ',\n",
       " 'devise ',\n",
       " 'diagnose ',\n",
       " 'diagnosis ',\n",
       " 'dictate ',\n",
       " 'dictator ',\n",
       " 'differentiate ',\n",
       " 'dignity ',\n",
       " 'dilemma ',\n",
       " 'dimension ',\n",
       " 'dip ',\n",
       " 'diplomat ',\n",
       " 'diplomatic ',\n",
       " 'directory ',\n",
       " 'disastrous ',\n",
       " 'discard ',\n",
       " 'discharge ',\n",
       " 'disclose ',\n",
       " 'disclosure ',\n",
       " 'discourse ',\n",
       " 'discretion ',\n",
       " 'discrimination ',\n",
       " 'dismissal ',\n",
       " 'displace ',\n",
       " 'disposal ',\n",
       " 'dispose ',\n",
       " 'dispute ',\n",
       " 'disrupt ',\n",
       " 'disruption ',\n",
       " 'dissolve ',\n",
       " 'distinction ',\n",
       " 'distinctive ',\n",
       " 'distort ',\n",
       " 'distress ',\n",
       " 'disturbing ',\n",
       " 'divert ',\n",
       " 'divine ',\n",
       " 'doctrine ',\n",
       " 'documentation ',\n",
       " 'domain ',\n",
       " 'dominance ',\n",
       " 'donor ',\n",
       " 'dose ',\n",
       " 'drain ',\n",
       " 'drift ',\n",
       " 'driving ',\n",
       " 'drown ',\n",
       " 'dual ',\n",
       " 'dub ',\n",
       " 'dumb ',\n",
       " 'duo ',\n",
       " 'dynamic ',\n",
       " 'eager ',\n",
       " 'earnings ',\n",
       " 'ease ',\n",
       " 'echo ',\n",
       " 'ecological ',\n",
       " 'educator ',\n",
       " 'effectiveness ',\n",
       " 'efficiency ',\n",
       " 'ego ',\n",
       " 'elaborate ',\n",
       " 'electoral ',\n",
       " 'elevate ',\n",
       " 'eligible ',\n",
       " 'elite ',\n",
       " 'embark ',\n",
       " 'embarrassment ',\n",
       " 'embassy ',\n",
       " 'embed ',\n",
       " 'embody ',\n",
       " 'emergence ',\n",
       " 'empirical ',\n",
       " 'empower ',\n",
       " 'enact ',\n",
       " 'encompass ',\n",
       " 'encouragement ',\n",
       " 'encouraging ',\n",
       " 'endeavour endless ',\n",
       " 'endorse ',\n",
       " 'endorsement ',\n",
       " 'endure ',\n",
       " 'enforce ',\n",
       " 'enforcement ',\n",
       " 'engagement ',\n",
       " 'engaging ',\n",
       " 'enquire ',\n",
       " 'enrich ',\n",
       " 'enrol ',\n",
       " 'ensue ',\n",
       " 'enterprise ',\n",
       " 'enthusiast ',\n",
       " 'entitle ',\n",
       " 'entity ',\n",
       " 'epidemic ',\n",
       " 'equality ',\n",
       " 'equation ',\n",
       " 'erect ',\n",
       " 'escalate ',\n",
       " 'essence ',\n",
       " 'establishment ',\n",
       " 'eternal ',\n",
       " 'evacuate ',\n",
       " 'evoke ',\n",
       " 'evolutionary ',\n",
       " 'exaggerate ',\n",
       " 'excellence ',\n",
       " 'exceptional ',\n",
       " 'excess ',\n",
       " 'exclusion ',\n",
       " 'exclusive ',\n",
       " 'exclusively ',\n",
       " 'execute ',\n",
       " 'execution ',\n",
       " 'exert ',\n",
       " 'exile ',\n",
       " 'exit ',\n",
       " 'expenditure ',\n",
       " 'experimental ',\n",
       " 'expire ',\n",
       " 'explicit ',\n",
       " 'explicitly ',\n",
       " 'exploitation ',\n",
       " 'explosive ',\n",
       " 'extract ',\n",
       " 'extremist ',\n",
       " 'facilitate ',\n",
       " 'faction ',\n",
       " 'faculty ',\n",
       " 'fade ',\n",
       " 'fairness ',\n",
       " 'fatal ',\n",
       " 'fate ',\n",
       " 'favourable ',\n",
       " 'feat ',\n",
       " 'feminist ',\n",
       " 'fibre ',\n",
       " 'fierce ',\n",
       " 'film maker ',\n",
       " 'filter ',\n",
       " 'fine ',\n",
       " 'firearm ',\n",
       " 'fit ',\n",
       " 'fixture ',\n",
       " 'flaw ',\n",
       " 'flawed ',\n",
       " 'flee ',\n",
       " 'fleet flesh ',\n",
       " 'flexibility ',\n",
       " 'flourish ',\n",
       " 'fluid ',\n",
       " 'footage ',\n",
       " 'foreigner ',\n",
       " 'forge ',\n",
       " 'formula ',\n",
       " 'formulate ',\n",
       " 'forth ',\n",
       " 'forthcoming ',\n",
       " 'foster ',\n",
       " 'fragile ',\n",
       " 'franchise ',\n",
       " 'frankly ',\n",
       " 'frustrated ',\n",
       " 'frustrating ',\n",
       " 'frustration ',\n",
       " 'functional ',\n",
       " 'fundraising ',\n",
       " 'funeral ',\n",
       " 'gallon ',\n",
       " 'gambling ',\n",
       " 'gathering ',\n",
       " 'gaze ',\n",
       " 'gear ',\n",
       " 'generic ',\n",
       " 'genocide ',\n",
       " 'glance ',\n",
       " 'glimpse ',\n",
       " 'glorious ',\n",
       " 'glory ',\n",
       " 'governance ',\n",
       " 'grace ',\n",
       " 'grasp ',\n",
       " 'grave for dead person ',\n",
       " 'grave serious ',\n",
       " 'gravity ',\n",
       " 'grid ',\n",
       " 'grief ',\n",
       " 'grin ',\n",
       " 'grind ',\n",
       " 'grip ',\n",
       " 'gross ',\n",
       " 'guerrilla ',\n",
       " 'guidance ',\n",
       " 'guilt ',\n",
       " 'gut ',\n",
       " 'hail ',\n",
       " 'halfway ',\n",
       " 'halt ',\n",
       " 'handful ',\n",
       " 'handling ',\n",
       " 'handy ',\n",
       " 'harassment ',\n",
       " 'hardware ',\n",
       " 'harmony ',\n",
       " 'harsh ',\n",
       " 'harvest ',\n",
       " 'hatred ',\n",
       " 'haunt ',\n",
       " 'hazard ',\n",
       " 'heighten ',\n",
       " 'heritage ',\n",
       " 'hierarchy ',\n",
       " 'high profile ',\n",
       " 'hint ',\n",
       " 'homeland ',\n",
       " 'hook ',\n",
       " 'hopeful horizon ',\n",
       " 'horn ',\n",
       " 'hostage ',\n",
       " 'hostile ',\n",
       " 'hostility ',\n",
       " 'humanitarian ',\n",
       " 'humanity ',\n",
       " 'humble ',\n",
       " 'hydrogen ',\n",
       " 'identification ',\n",
       " 'ideological ',\n",
       " 'ideology ',\n",
       " 'idiot ',\n",
       " 'ignorance ',\n",
       " 'imagery ',\n",
       " 'immense ',\n",
       " 'imminent ',\n",
       " 'implementation ',\n",
       " 'imprison ',\n",
       " 'imprisonment ',\n",
       " 'inability ',\n",
       " 'inadequate ',\n",
       " 'inappropriate ',\n",
       " 'incidence ',\n",
       " 'inclined ',\n",
       " 'inclusion ',\n",
       " 'incur ',\n",
       " 'indicator ',\n",
       " 'indictment ',\n",
       " 'indigenous ',\n",
       " 'induce ',\n",
       " 'indulge ',\n",
       " 'inequality ',\n",
       " 'infamous ',\n",
       " 'infant ',\n",
       " 'infect ',\n",
       " 'inflict ',\n",
       " 'influential ',\n",
       " 'inherent ',\n",
       " 'inhibit ',\n",
       " 'initiate ',\n",
       " 'inject ',\n",
       " 'injection ',\n",
       " 'injustice ',\n",
       " 'inmate ',\n",
       " 'insertion ',\n",
       " 'insider ',\n",
       " 'inspect ',\n",
       " 'inspection ',\n",
       " 'inspiration ',\n",
       " 'instinct ',\n",
       " 'institutional ',\n",
       " 'instruct ',\n",
       " 'instrumental ',\n",
       " 'insufficient ',\n",
       " 'insult ',\n",
       " 'intact ',\n",
       " 'intake ',\n",
       " 'integral ',\n",
       " 'integrated ',\n",
       " 'integration ',\n",
       " 'integrity ',\n",
       " 'intellectual ',\n",
       " 'intensify ',\n",
       " 'intensity ',\n",
       " 'intensive ',\n",
       " 'intent ',\n",
       " 'interactive ',\n",
       " 'interface ',\n",
       " 'interfere ',\n",
       " 'interim ',\n",
       " 'interior ',\n",
       " 'intermediate ',\n",
       " 'intervene ',\n",
       " 'intervention ',\n",
       " 'intimate ',\n",
       " 'intriguing ',\n",
       " 'investigator ',\n",
       " 'invisible ',\n",
       " 'invoke ',\n",
       " 'involvement ',\n",
       " 'ironic ',\n",
       " 'ironically ',\n",
       " 'irony ',\n",
       " 'irrelevant ',\n",
       " 'isolation ',\n",
       " 'judicial ',\n",
       " 'junction ',\n",
       " 'jurisdiction ',\n",
       " 'just ',\n",
       " 'justification ',\n",
       " 'kidnap ',\n",
       " 'kidney ',\n",
       " 'kingdom ',\n",
       " 'lad ',\n",
       " 'landlord ',\n",
       " 'landmark ',\n",
       " 'lap ',\n",
       " 'large scale ',\n",
       " 'laser ',\n",
       " 'latter ',\n",
       " 'lawn ',\n",
       " 'lawsuit ',\n",
       " 'layout ',\n",
       " 'leak ',\n",
       " 'leap ',\n",
       " 'legacy ',\n",
       " 'legendary ',\n",
       " 'legislation ',\n",
       " 'legislative ',\n",
       " 'legislature ',\n",
       " 'legitimate ',\n",
       " 'lengthy ',\n",
       " 'lesbian ',\n",
       " 'lesser ',\n",
       " 'lethal ',\n",
       " 'liable ',\n",
       " 'liberal ',\n",
       " 'liberation ',\n",
       " 'liberty ',\n",
       " 'license ',\n",
       " 'lifelong ',\n",
       " 'likelihood ',\n",
       " 'limb ',\n",
       " 'linear ',\n",
       " 'line ',\n",
       " 'linger ',\n",
       " 'listing ',\n",
       " 'literacy ',\n",
       " 'liver ',\n",
       " 'lobby ',\n",
       " 'log ',\n",
       " 'logic ',\n",
       " 'long standing ',\n",
       " 'long time ',\n",
       " 'loom ',\n",
       " 'loop ',\n",
       " 'loyalty ',\n",
       " 'machinery magical ',\n",
       " 'magistrate ',\n",
       " 'magnetic ',\n",
       " 'magnitude ',\n",
       " 'mainland ',\n",
       " 'mainstream ',\n",
       " 'maintenance ',\n",
       " 'mandate ',\n",
       " 'mandatory ',\n",
       " 'manifest ',\n",
       " 'manipulate ',\n",
       " 'manipulation ',\n",
       " 'manuscript ',\n",
       " 'march ',\n",
       " 'marginal ',\n",
       " 'marine ',\n",
       " 'marketplace ',\n",
       " 'mask ',\n",
       " 'massacre ',\n",
       " 'mathematical ',\n",
       " 'mature ',\n",
       " 'maximize ',\n",
       " 'meaningful ',\n",
       " 'meantime ',\n",
       " 'medieval ',\n",
       " 'meditation ',\n",
       " 'melody ',\n",
       " 'memo ',\n",
       " 'memoir ',\n",
       " 'memorial ',\n",
       " 'mentor ',\n",
       " 'merchant ',\n",
       " 'mercy ',\n",
       " 'mere ',\n",
       " 'merely ',\n",
       " 'merge ',\n",
       " 'merger ',\n",
       " 'merit ',\n",
       " 'methodology ',\n",
       " 'midst ',\n",
       " 'migration ',\n",
       " 'militant ',\n",
       " 'militia ',\n",
       " 'mill ',\n",
       " 'minimal ',\n",
       " 'minimize ',\n",
       " 'mining ',\n",
       " 'ministry ',\n",
       " 'minute ',\n",
       " 'miracle ',\n",
       " 'misery ',\n",
       " 'misleading ',\n",
       " 'missile ',\n",
       " 'mob ',\n",
       " 'mobility ',\n",
       " 'mobilize ',\n",
       " 'moderate ',\n",
       " 'modification ',\n",
       " 'momentum ',\n",
       " 'monk ',\n",
       " 'monopoly ',\n",
       " 'morality ',\n",
       " 'motive ',\n",
       " 'motorist ',\n",
       " 'municipal ',\n",
       " 'mutual ',\n",
       " 'namely ',\n",
       " 'nationwide ',\n",
       " 'naval ',\n",
       " 'neglect neighbouring ',\n",
       " 'nest ',\n",
       " 'net ',\n",
       " 'newsletter ',\n",
       " 'niche ',\n",
       " 'noble ',\n",
       " 'nod ',\n",
       " 'nominate ',\n",
       " 'nomination ',\n",
       " 'nominee ',\n",
       " 'nonetheless ',\n",
       " 'non profit ',\n",
       " 'nonsense ',\n",
       " 'noon ',\n",
       " 'notable ',\n",
       " 'notably ',\n",
       " 'notify ',\n",
       " 'notorious ',\n",
       " 'novel ',\n",
       " 'nursery ',\n",
       " 'objection ',\n",
       " 'oblige ',\n",
       " 'obsess ',\n",
       " 'obsession ',\n",
       " 'occasional ',\n",
       " 'occurrence ',\n",
       " 'odds ',\n",
       " 'offering ',\n",
       " 'offspring ',\n",
       " 'operational ',\n",
       " 'opt ',\n",
       " 'optical ',\n",
       " 'optimism ',\n",
       " 'oral ',\n",
       " 'organizational ',\n",
       " 'orientation ',\n",
       " 'originate ',\n",
       " 'outbreak ',\n",
       " 'outing ',\n",
       " 'outlet ',\n",
       " 'outlook ',\n",
       " 'outrage ',\n",
       " 'outsider ',\n",
       " 'overlook ',\n",
       " 'overly ',\n",
       " 'oversee ',\n",
       " 'overturn ',\n",
       " 'overwhelm ',\n",
       " 'overwhelming ',\n",
       " 'pad ',\n",
       " 'parameter ',\n",
       " 'parental ',\n",
       " 'parish ',\n",
       " 'parliamentary ',\n",
       " 'partial ',\n",
       " 'partially ',\n",
       " 'passing ',\n",
       " 'passive ',\n",
       " 'pastor ',\n",
       " 'patch ',\n",
       " 'patent ',\n",
       " 'pathway ',\n",
       " 'patrol ',\n",
       " 'patron ',\n",
       " 'peak ',\n",
       " 'peasant ',\n",
       " 'peculiar ',\n",
       " 'persist ',\n",
       " 'persistent ',\n",
       " 'personnel petition ',\n",
       " 'philosopher ',\n",
       " 'philosophical ',\n",
       " 'physician ',\n",
       " 'pioneer ',\n",
       " 'pipeline ',\n",
       " 'pirate ',\n",
       " 'pit ',\n",
       " 'plea ',\n",
       " 'plead ',\n",
       " 'pledge ',\n",
       " 'plug ',\n",
       " 'plunge ',\n",
       " 'pole ',\n",
       " 'poll ',\n",
       " 'pond ',\n",
       " 'pop ',\n",
       " 'portfolio ',\n",
       " 'portray ',\n",
       " 'postpone ',\n",
       " 'post war ',\n",
       " 'practitioner ',\n",
       " 'preach ',\n",
       " 'precedent ',\n",
       " 'precision ',\n",
       " 'predator ',\n",
       " 'predecessor ',\n",
       " 'predominantly ',\n",
       " 'pregnancy ',\n",
       " 'prejudice ',\n",
       " 'preliminary ',\n",
       " 'premier ',\n",
       " 'premise ',\n",
       " 'premium ',\n",
       " 'prescribe ',\n",
       " 'prescription ',\n",
       " 'presently ',\n",
       " 'preservation ',\n",
       " 'preside ',\n",
       " 'presidency ',\n",
       " 'presidential ',\n",
       " 'prestigious ',\n",
       " 'presumably ',\n",
       " 'presume ',\n",
       " 'prevail ',\n",
       " 'prevalence ',\n",
       " 'prevention ',\n",
       " 'prey ',\n",
       " 'principal ',\n",
       " 'privatization ',\n",
       " 'privilege ',\n",
       " 'probe ',\n",
       " 'problematic ',\n",
       " 'proceedings ',\n",
       " 'proceeds ',\n",
       " 'processing ',\n",
       " 'processor ',\n",
       " 'proclaim ',\n",
       " 'productive ',\n",
       " 'productivity ',\n",
       " 'profitable ',\n",
       " 'profound ',\n",
       " 'projection ',\n",
       " 'prominent ',\n",
       " 'ounced ',\n",
       " 'propaganda ',\n",
       " 'proposition ',\n",
       " 'prosecute ',\n",
       " 'prosecution ',\n",
       " 'prosecutor ',\n",
       " 'prosperity ',\n",
       " 'protective ',\n",
       " 'protocol ',\n",
       " 'province ',\n",
       " 'provincial ',\n",
       " 'provision ',\n",
       " 'provoke',\n",
       " 'provoke',\n",
       " 'psychiatric ',\n",
       " 'pulse',\n",
       " 'punch ',\n",
       " 'query ',\n",
       " 'rally',\n",
       " 'ranking ',\n",
       " 'ratio ',\n",
       " 'reasoning ',\n",
       " 'referendum ',\n",
       " 'refuge ',\n",
       " 'rejection',\n",
       " 'residue ',\n",
       " 'resignation ',\n",
       " 'resistance ',\n",
       " 'respective ',\n",
       " 'respectively ',\n",
       " 'restoration ',\n",
       " 'restraint ',\n",
       " 'resume ',\n",
       " 'retreat ',\n",
       " 'retrieve ',\n",
       " 'revelation ',\n",
       " 'reven ',\n",
       " 'reverse ',\n",
       " 'revival ',\n",
       " 'revive ',\n",
       " 'revolutionary ',\n",
       " 'rhetoric ',\n",
       " 'rifle ',\n",
       " 'riot ',\n",
       " 'rip ',\n",
       " 'ritual ',\n",
       " 'robust ',\n",
       " 'rock ',\n",
       " 'rod ',\n",
       " 'rotate ',\n",
       " 'rotation ',\n",
       " 'ruling ',\n",
       " 'rumour ',\n",
       " 'sack ',\n",
       " 'sacred ',\n",
       " 'sacrifice ',\n",
       " 'saint ',\n",
       " 'sake ',\n",
       " 'sanction ',\n",
       " 'say ',\n",
       " 'scattered ',\n",
       " 'sceptical ',\n",
       " 'scope ',\n",
       " 'screw ',\n",
       " 'scrutiny ',\n",
       " 'seal ',\n",
       " 'secular ',\n",
       " 'seemingly ',\n",
       " 'segment ',\n",
       " 'seize ',\n",
       " 'seldom ',\n",
       " 'selective ',\n",
       " 'senator ',\n",
       " 'sensation ',\n",
       " 'sensitivity ',\n",
       " 'sentiment ',\n",
       " 'separation ',\n",
       " 'serial ',\n",
       " 'settlement ',\n",
       " 'set ',\n",
       " 'sexuality ',\n",
       " 'shareholder ',\n",
       " 'shatter ',\n",
       " 'shed ',\n",
       " 'sheer ',\n",
       " ...]"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict_A2 = vocab['A2'][0:860].to_dict()\n",
    "dict_B1 = vocab['B1'][0:797].to_dict()\n",
    "dict_B2 = vocab['B2'][0:].to_dict()\n",
    "dict_C1 = vocab['C1'][0:1117].to_dict()\n",
    "dict_A2 = list(dict_A2.values())\n",
    "dict_B1 = list(dict_B1.values())\n",
    "dict_B2 = list(dict_B2.values())\n",
    "dict_C1 = list(dict_C1.values()) #all_dict\n",
    "all_dict = [dict_A2, dict_B1, dict_B2, dict_C1]\n",
    "all_dict[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Выгрузка субтитров и сопоставление с названиями фильмов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество названий фильмов в папке с субтитрами: 279\n"
     ]
    }
   ],
   "source": [
    "# загрузим в список имена файлов из папки с субтитрами\n",
    "films_call = os.listdir(path='.\\Subtitles_all\\Subtitles')\n",
    "# определим количество названий фильмов\n",
    "print(f'Количество названий фильмов в папке с субтитрами: {len(films_call)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество фильмов, имеющих метку и субтитры: 229\n"
     ]
    }
   ],
   "source": [
    "# проверим для скольких фильмов, имеющих метку из таблицы, предоставлены субтитры \n",
    "films_filtr = set(films_call) & set(data['Movie'] + '.srt')\n",
    "print(f'Количество фильмов, имеющих метку и субтитры: {len(films_filtr)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подготовим фильтры для субтитров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Введем переменные для очистки текста\n",
    "HTML = r'<.*?>' # html тэги меняем на пробел\n",
    "TAG = r'{.*?}' # тэги меняем на пробел\n",
    "COMMENTS = r'[\\(\\[][A-Za-z ]+[\\)\\]]' # комменты в скобках меняем на пробел\n",
    "LETTERS = r'[^a-zA-Z\\.,!? ]' # все что не буквы меняем на пробел \n",
    "SPACES = r'([ ])\\1+' # повторяющиеся пробелы меняем на один пробел\n",
    "SYMB = r'[^\\w\\s]' # знаки припинания\n",
    "LONG = r'\\b\\w{1,2}\\b'# удаление слов менее трех символов\n",
    "DOTS = r'[\\.]+' # многоточие меняем на точку\n",
    "# SYMB = r\"[^\\w\\d'\\s]\" # знаки препинания кроме апострофа\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "# напишем функцию для очистки субтитров\n",
    "def clean_subs(subs):\n",
    "    subs = subs[1:] # удаляем первый рекламный субтитр\n",
    "    txt = re.sub(HTML, ' ', subs.text) # html тэги меняем на пробел\n",
    "    txt = re.sub(COMMENTS, ' ', txt) # комменты в скобках меняем на пробел\n",
    "    txt = re.sub(LETTERS, ' ', txt) # все что не буквы меняем на пробел\n",
    "    txt = re.sub(DOTS, r'.', txt) # многоточие меняем на точку\n",
    "    txt = re.sub(SPACES, r'\\1', txt) # повторяющиеся пробелы меняем на один пробел\n",
    "    #txt = re.sub(LONG, r'\\b\\w{1,2}\\b', '', txt) # удаление менее трех символов\n",
    "    txt = re.sub(SYMB, '', txt) # знаки препинания кроме апострофа на пустую строку\n",
    "    txt = re.sub('www', '', txt) # кое-где остаётся www, то же меняем на пустую строку\n",
    "    txt = txt.lstrip() # обрезка пробелов слева\n",
    "    txt = txt.encode('ascii', 'ignore').decode() # удаляем все что не ascii символы   \n",
    "    txt = txt.lower() # текст в нижний регистр\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Подготовим функцию для лемматизации и сортировки используя для каждокй категории свой словарь, подгодтовленный ранее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция возвращающая количество уникальных лемм\n",
    "# в тексте субтитров по каждому уровню\n",
    "def lemma_count(lemmas, oxf, cat):\n",
    "    func_dict = {\"A2\": 1, \n",
    "                 \"B1\": 2, \n",
    "                 \"B2\": 3, \n",
    "                 \"C1\": 4}\n",
    "    level = func_dict[cat]\n",
    "    oxf_word_list = oxf[level]#.split()\n",
    "    words = [lemma for lemma in lemmas if lemma in oxf_word_list]\n",
    "\n",
    "    return len(set(words))\n",
    "# загрузим для каждого фильма субтитры с использованием библиотеки pysrt\n",
    "\n",
    "for film in films_filtr:\n",
    "    #try: \n",
    "    subs = pysrt.open(f'.\\Subtitles_all\\Subtitles\\{film}', encoding='iso-8859-1')\n",
    "    #except:\n",
    "    #    subs = pysrt.open(f'.\\Subtitles_all\\Subtitles{film}', encoding='iso-8859-1')\n",
    "    # вызов функии для очистки текста\n",
    "\n",
    "    cln_subs = clean_subs(subs)\n",
    "    data.loc[data['Movie'] == film[:-4], 'subs'] = cln_subs\n",
    "    # используем библиотеку spacy для лемматизации \n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(cln_subs)\n",
    "    lemma_list = [token.lemma_ for token in doc]\n",
    "    for lvl in ['A2', 'B1', 'B2', 'C1']:\n",
    "        data.loc[data['Movie'] == film[:-4], lvl+'_lemma_cnt'] = lemma_count(lemma_list, dict_A2, lvl)\n",
    "    dffr_A2 = pd.DataFrame(data, columns=['id', 'Level', 'Movie','A2_lemma_cnt', 'B1_lemma_cnt', 'B2_lemma_cnt', 'C1_lemma_cnt' , 'subs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Level</th>\n",
       "      <th>Movie</th>\n",
       "      <th>A2_lemma_cnt</th>\n",
       "      <th>B1_lemma_cnt</th>\n",
       "      <th>B2_lemma_cnt</th>\n",
       "      <th>C1_lemma_cnt</th>\n",
       "      <th>subs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ben on phone michelle please don t hang up jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>i ll be right with you so cameron here you go ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>should we help him he s due in the lists in tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>get to it black eyes open wide it s time to te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>where the caravan camels roam where it s flat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>236</td>\n",
       "      <td>4</td>\n",
       "      <td>Matilda(2022)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>237</td>\n",
       "      <td>2</td>\n",
       "      <td>Bullet train</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>238</td>\n",
       "      <td>3</td>\n",
       "      <td>Thor: love and thunder</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>239</td>\n",
       "      <td>3</td>\n",
       "      <td>Lightyear</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>240</td>\n",
       "      <td>2</td>\n",
       "      <td>The Grinch</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  Level                             Movie  A2_lemma_cnt  B1_lemma_cnt  \\\n",
       "0      0      2         10_Cloverfield_lane(2016)           3.0           6.0   \n",
       "1      1      2  10_things_I_hate_about_you(1999)           5.0           5.0   \n",
       "2      2      3              A_knights_tale(2001)           4.0           4.0   \n",
       "3      3      3              A_star_is_born(2018)           3.0           4.0   \n",
       "4      4      1                     Aladdin(1992)           5.0           6.0   \n",
       "..   ...    ...                               ...           ...           ...   \n",
       "236  236      4                     Matilda(2022)           4.0           4.0   \n",
       "237  237      2                      Bullet train           4.0           4.0   \n",
       "238  238      3            Thor: love and thunder           4.0           4.0   \n",
       "239  239      3                         Lightyear           4.0           4.0   \n",
       "240  240      2                        The Grinch           4.0           4.0   \n",
       "\n",
       "     B2_lemma_cnt  C1_lemma_cnt  \\\n",
       "0             4.0           6.0   \n",
       "1             5.0           7.0   \n",
       "2             3.0           6.0   \n",
       "3             3.0           7.0   \n",
       "4             4.0           6.0   \n",
       "..            ...           ...   \n",
       "236           4.0           4.0   \n",
       "237           4.0           4.0   \n",
       "238           4.0           4.0   \n",
       "239           4.0           4.0   \n",
       "240           4.0           4.0   \n",
       "\n",
       "                                                  subs  \n",
       "0    ben on phone michelle please don t hang up jus...  \n",
       "1    i ll be right with you so cameron here you go ...  \n",
       "2    should we help him he s due in the lists in tw...  \n",
       "3    get to it black eyes open wide it s time to te...  \n",
       "4    where the caravan camels roam where it s flat ...  \n",
       "..                                                 ...  \n",
       "236                                                NaN  \n",
       "237                                                NaN  \n",
       "238                                                NaN  \n",
       "239                                                NaN  \n",
       "240                                                NaN  \n",
       "\n",
       "[241 rows x 8 columns]"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffr_A2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Для B1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "for film in films_filtr:\n",
    "    subs = pysrt.open(f'.\\Subtitles_all\\Subtitles\\{film}', encoding='iso-8859-1')\n",
    "    cln_subs = clean_subs(subs)\n",
    "    data.loc[data['Movie'] == film[:-4], 'subs'] = cln_subs\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(cln_subs)\n",
    "    lemma_list = [token.lemma_ for token in doc]\n",
    "    for lvl in ['A2', 'B1', 'B2', 'C1']:\n",
    "        data.loc[data['Movie'] == film[:-4], lvl+'_lemma_cnt'] = lemma_count(lemma_list, dict_B1, lvl)\n",
    "    dffr_B1 = pd.DataFrame(data, columns=['id', 'Movie', 'A2_lemma_cnt', 'B1_lemma_cnt', 'B2_lemma_cnt', 'C1_lemma_cnt', 'subs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Movie</th>\n",
       "      <th>A2_lemma_cnt</th>\n",
       "      <th>B1_lemma_cnt</th>\n",
       "      <th>B2_lemma_cnt</th>\n",
       "      <th>C1_lemma_cnt</th>\n",
       "      <th>subs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>ben on phone michelle please don t hang up jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>i ll be right with you so cameron here you go ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>should we help him he s due in the lists in tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>get to it black eyes open wide it s time to te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>where the caravan camels roam where it s flat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>236</td>\n",
       "      <td>Matilda(2022)</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>237</td>\n",
       "      <td>Bullet train</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>238</td>\n",
       "      <td>Thor: love and thunder</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>239</td>\n",
       "      <td>Lightyear</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>240</td>\n",
       "      <td>The Grinch</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                             Movie  A2_lemma_cnt  B1_lemma_cnt  \\\n",
       "0      0         10_Cloverfield_lane(2016)      3.000000     10.000000   \n",
       "1      1  10_things_I_hate_about_you(1999)      4.000000      8.000000   \n",
       "2      2              A_knights_tale(2001)      3.000000      7.000000   \n",
       "3      3              A_star_is_born(2018)      3.000000      8.000000   \n",
       "4      4                     Aladdin(1992)      4.000000     10.000000   \n",
       "..   ...                               ...           ...           ...   \n",
       "236  236                     Matilda(2022)      5.873391      5.873391   \n",
       "237  237                      Bullet train      5.873391      5.873391   \n",
       "238  238            Thor: love and thunder      5.873391      5.873391   \n",
       "239  239                         Lightyear      5.873391      5.873391   \n",
       "240  240                        The Grinch      5.873391      5.873391   \n",
       "\n",
       "     B2_lemma_cnt  C1_lemma_cnt  \\\n",
       "0        5.000000      6.000000   \n",
       "1        5.000000      9.000000   \n",
       "2        4.000000      5.000000   \n",
       "3        3.000000      8.000000   \n",
       "4        6.000000      9.000000   \n",
       "..            ...           ...   \n",
       "236      5.873391      5.873391   \n",
       "237      5.873391      5.873391   \n",
       "238      5.873391      5.873391   \n",
       "239      5.873391      5.873391   \n",
       "240      5.873391      5.873391   \n",
       "\n",
       "                                                  subs  \n",
       "0    ben on phone michelle please don t hang up jus...  \n",
       "1    i ll be right with you so cameron here you go ...  \n",
       "2    should we help him he s due in the lists in tw...  \n",
       "3    get to it black eyes open wide it s time to te...  \n",
       "4    where the caravan camels roam where it s flat ...  \n",
       "..                                                 ...  \n",
       "236                                                NaN  \n",
       "237                                                NaN  \n",
       "238                                                NaN  \n",
       "239                                                NaN  \n",
       "240                                                NaN  \n",
       "\n",
       "[241 rows x 7 columns]"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffr_B1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Для B2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "for film in films_filtr:\n",
    "    subs = pysrt.open(f'.\\Subtitles_all\\Subtitles\\{film}', encoding='iso-8859-1')\n",
    "    cln_subs = clean_subs(subs)\n",
    "    data.loc[data['Movie'] == film[:-4], 'subs'] = cln_subs\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(cln_subs)\n",
    "    lemma_list = [token.lemma_ for token in doc]\n",
    "    for lvl in ['A2', 'B1', 'B2', 'C1']:\n",
    "        data.loc[data['Movie'] == film[:-4], lvl+'_lemma_cnt'] = lemma_count(lemma_list, dict_B2, lvl)\n",
    "        dffr_B2 = pd.DataFrame(data, columns=['id', 'Movie', 'A2_lemma_cnt', 'B1_lemma_cnt', 'B2_lemma_cnt', 'C1_lemma_cnt', 'subs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Movie</th>\n",
       "      <th>A2_lemma_cnt</th>\n",
       "      <th>B1_lemma_cnt</th>\n",
       "      <th>B2_lemma_cnt</th>\n",
       "      <th>C1_lemma_cnt</th>\n",
       "      <th>subs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>ben on phone michelle please don t hang up jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>i ll be right with you so cameron here you go ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>should we help him he s due in the lists in tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>get to it black eyes open wide it s time to te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>where the caravan camels roam where it s flat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>236</td>\n",
       "      <td>Matilda(2022)</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>237</td>\n",
       "      <td>Bullet train</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>238</td>\n",
       "      <td>Thor: love and thunder</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>239</td>\n",
       "      <td>Lightyear</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>240</td>\n",
       "      <td>The Grinch</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                             Movie  A2_lemma_cnt  B1_lemma_cnt  \\\n",
       "0      0         10_Cloverfield_lane(2016)      6.000000      5.000000   \n",
       "1      1  10_things_I_hate_about_you(1999)      8.000000      9.000000   \n",
       "2      2              A_knights_tale(2001)      6.000000      5.000000   \n",
       "3      3              A_star_is_born(2018)      5.000000      8.000000   \n",
       "4      4                     Aladdin(1992)      9.000000      7.000000   \n",
       "..   ...                               ...           ...           ...   \n",
       "236  236                     Matilda(2022)      5.851931      5.851931   \n",
       "237  237                      Bullet train      5.851931      5.851931   \n",
       "238  238            Thor: love and thunder      5.851931      5.851931   \n",
       "239  239                         Lightyear      5.851931      5.851931   \n",
       "240  240                        The Grinch      5.851931      5.851931   \n",
       "\n",
       "     B2_lemma_cnt  C1_lemma_cnt  \\\n",
       "0        7.000000      7.000000   \n",
       "1        9.000000      6.000000   \n",
       "2        5.000000      6.000000   \n",
       "3        5.000000      6.000000   \n",
       "4        8.000000      7.000000   \n",
       "..            ...           ...   \n",
       "236      5.851931      5.851931   \n",
       "237      5.851931      5.851931   \n",
       "238      5.851931      5.851931   \n",
       "239      5.851931      5.851931   \n",
       "240      5.851931      5.851931   \n",
       "\n",
       "                                                  subs  \n",
       "0    ben on phone michelle please don t hang up jus...  \n",
       "1    i ll be right with you so cameron here you go ...  \n",
       "2    should we help him he s due in the lists in tw...  \n",
       "3    get to it black eyes open wide it s time to te...  \n",
       "4    where the caravan camels roam where it s flat ...  \n",
       "..                                                 ...  \n",
       "236                                                NaN  \n",
       "237                                                NaN  \n",
       "238                                                NaN  \n",
       "239                                                NaN  \n",
       "240                                                NaN  \n",
       "\n",
       "[241 rows x 7 columns]"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffr_B2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Для C1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "for film in films_filtr:\n",
    "    subs = pysrt.open(f'.\\Subtitles_all\\Subtitles\\{film}', encoding='iso-8859-1')\n",
    "    cln_subs = clean_subs(subs)\n",
    "    data.loc[data['Movie'] == film[:-4], 'subs'] = cln_subs\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(cln_subs)\n",
    "    lemma_list = [token.lemma_ for token in doc]\n",
    "    for lvl in ['A2', 'B1', 'B2', 'C1']:\n",
    "        data.loc[data['Movie'] == film[:-4], lvl+'_lemma_cnt'] = lemma_count(lemma_list, dict_C1, lvl)\n",
    "        dffr_C1 = pd.DataFrame(data, columns=['id', 'Movie', 'A2_lemma_cnt', 'B1_lemma_cnt', 'B2_lemma_cnt', 'C1_lemma_cnt', 'subs'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Movie</th>\n",
       "      <th>A2_lemma_cnt</th>\n",
       "      <th>B1_lemma_cnt</th>\n",
       "      <th>B2_lemma_cnt</th>\n",
       "      <th>C1_lemma_cnt</th>\n",
       "      <th>subs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>ben on phone michelle please don t hang up jus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>i ll be right with you so cameron here you go ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>should we help him he s due in the lists in tw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>get to it black eyes open wide it s time to te...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>where the caravan camels roam where it s flat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>236</td>\n",
       "      <td>Matilda(2022)</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>237</td>\n",
       "      <td>Bullet train</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>238</td>\n",
       "      <td>Thor: love and thunder</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>239</td>\n",
       "      <td>Lightyear</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>240</td>\n",
       "      <td>The Grinch</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                             Movie  A2_lemma_cnt  B1_lemma_cnt  \\\n",
       "0      0         10_Cloverfield_lane(2016)      8.000000      3.000000   \n",
       "1      1  10_things_I_hate_about_you(1999)      6.000000      4.000000   \n",
       "2      2              A_knights_tale(2001)      5.000000      4.000000   \n",
       "3      3              A_star_is_born(2018)      6.000000      3.000000   \n",
       "4      4                     Aladdin(1992)      7.000000      6.000000   \n",
       "..   ...                               ...           ...           ...   \n",
       "236  236                     Matilda(2022)      4.639485      4.639485   \n",
       "237  237                      Bullet train      4.639485      4.639485   \n",
       "238  238            Thor: love and thunder      4.639485      4.639485   \n",
       "239  239                         Lightyear      4.639485      4.639485   \n",
       "240  240                        The Grinch      4.639485      4.639485   \n",
       "\n",
       "     B2_lemma_cnt  C1_lemma_cnt  \\\n",
       "0        4.000000      5.000000   \n",
       "1        5.000000      5.000000   \n",
       "2        4.000000      4.000000   \n",
       "3        4.000000      4.000000   \n",
       "4        6.000000      5.000000   \n",
       "..            ...           ...   \n",
       "236      4.639485      4.639485   \n",
       "237      4.639485      4.639485   \n",
       "238      4.639485      4.639485   \n",
       "239      4.639485      4.639485   \n",
       "240      4.639485      4.639485   \n",
       "\n",
       "                                                  subs  \n",
       "0    ben on phone michelle please don t hang up jus...  \n",
       "1    i ll be right with you so cameron here you go ...  \n",
       "2    should we help him he s due in the lists in tw...  \n",
       "3    get to it black eyes open wide it s time to te...  \n",
       "4    where the caravan camels roam where it s flat ...  \n",
       "..                                                 ...  \n",
       "236                                                NaN  \n",
       "237                                                NaN  \n",
       "238                                                NaN  \n",
       "239                                                NaN  \n",
       "240                                                NaN  \n",
       "\n",
       "[241 rows x 7 columns]"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dffr_C1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Проведем сбор датафрейма на основе лемматизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Соберем новый фрейм и переименуем столбцы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_featch = pd.concat([dffr_A2.iloc[:, -1: ], dffr_A2.iloc[:, 1:7],  dffr_B1.iloc[:, 2:6], dffr_B2.iloc[:, 2:6], dffr_C1.iloc[:, 2:6]], ignore_index=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_featch.rename(columns={new_featch.columns[0] : 'subs', new_featch.columns[1] : 'Level', new_featch.columns[2] : 'Movie', new_featch.columns[3] : 'A2_A2', new_featch.columns[4] : 'B1_A2', new_featch.columns[5] : 'B2_A2', new_featch.columns[6] : 'C1_A2', new_featch.columns[7] : 'A2_B1', new_featch.columns[8] : 'B1_B1', new_featch.columns[9] : 'B2_B1', new_featch.columns[10] : 'C1_B1', new_featch.columns[11] : 'A2_B2', new_featch.columns[12] : 'B1_B2', new_featch.columns[13] : 'B2_B2', new_featch.columns[14] : 'C1_B2', new_featch.columns[15] : 'A2_C1', new_featch.columns[16] : 'B1_C1', new_featch.columns[17] : 'B2_C1',new_featch.columns[18] : 'C1_C1', new_featch.columns[0] : 'subs', new_featch.columns[1] : 'Level', new_featch.columns[2] : 'Movie', new_featch.columns[3] : 'A2_A2', new_featch.columns[4] : 'B1_A2', new_featch.columns[5] : 'B2_A2', new_featch.columns[6] : 'C1_A2', new_featch.columns[7] : 'A2_B1', new_featch.columns[8] : 'B1_B1', new_featch.columns[9] : 'B2_B1', new_featch.columns[10] : 'C1_B1', new_featch.columns[11] : 'A2_B2', new_featch.columns[12] : 'B1_B2', new_featch.columns[13] : 'B2_B2', new_featch.columns[14] : 'C1_B2', new_featch.columns[15] : 'A2_C1', new_featch.columns[16] : 'B1_C1', new_featch.columns[17] : 'B2_C1',new_featch.columns[18] : 'C1_C1'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subs</th>\n",
       "      <th>Level</th>\n",
       "      <th>Movie</th>\n",
       "      <th>A2_A2</th>\n",
       "      <th>B1_A2</th>\n",
       "      <th>B2_A2</th>\n",
       "      <th>C1_A2</th>\n",
       "      <th>A2_B1</th>\n",
       "      <th>B1_B1</th>\n",
       "      <th>B2_B1</th>\n",
       "      <th>C1_B1</th>\n",
       "      <th>A2_B2</th>\n",
       "      <th>B1_B2</th>\n",
       "      <th>B2_B2</th>\n",
       "      <th>C1_B2</th>\n",
       "      <th>A2_C1</th>\n",
       "      <th>B1_C1</th>\n",
       "      <th>B2_C1</th>\n",
       "      <th>C1_C1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ben on phone michelle please don t hang up jus...</td>\n",
       "      <td>2</td>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i ll be right with you so cameron here you go ...</td>\n",
       "      <td>2</td>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>should we help him he s due in the lists in tw...</td>\n",
       "      <td>3</td>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get to it black eyes open wide it s time to te...</td>\n",
       "      <td>3</td>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>where the caravan camels roam where it s flat ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                subs  Level  \\\n",
       "0  ben on phone michelle please don t hang up jus...      2   \n",
       "1  i ll be right with you so cameron here you go ...      2   \n",
       "2  should we help him he s due in the lists in tw...      3   \n",
       "3  get to it black eyes open wide it s time to te...      3   \n",
       "4  where the caravan camels roam where it s flat ...      1   \n",
       "\n",
       "                              Movie  A2_A2  B1_A2  B2_A2  C1_A2  A2_B1  B1_B1  \\\n",
       "0         10_Cloverfield_lane(2016)    3.0    6.0    4.0    6.0    3.0   10.0   \n",
       "1  10_things_I_hate_about_you(1999)    5.0    5.0    5.0    7.0    4.0    8.0   \n",
       "2              A_knights_tale(2001)    4.0    4.0    3.0    6.0    3.0    7.0   \n",
       "3              A_star_is_born(2018)    3.0    4.0    3.0    7.0    3.0    8.0   \n",
       "4                     Aladdin(1992)    5.0    6.0    4.0    6.0    4.0   10.0   \n",
       "\n",
       "   B2_B1  C1_B1  A2_B2  B1_B2  B2_B2  C1_B2  A2_C1  B1_C1  B2_C1  C1_C1  \n",
       "0    5.0    6.0    6.0    5.0    7.0    7.0    8.0    3.0    4.0    5.0  \n",
       "1    5.0    9.0    8.0    9.0    9.0    6.0    6.0    4.0    5.0    5.0  \n",
       "2    4.0    5.0    6.0    5.0    5.0    6.0    5.0    4.0    4.0    4.0  \n",
       "3    3.0    8.0    5.0    8.0    5.0    6.0    6.0    3.0    4.0    4.0  \n",
       "4    6.0    9.0    9.0    7.0    8.0    7.0    7.0    6.0    6.0    5.0  "
      ]
     },
     "execution_count": 494,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_featch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Обработаем пропуски"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subs</th>\n",
       "      <th>Level</th>\n",
       "      <th>Movie</th>\n",
       "      <th>A2_A2</th>\n",
       "      <th>B1_A2</th>\n",
       "      <th>B2_A2</th>\n",
       "      <th>C1_A2</th>\n",
       "      <th>A2_B1</th>\n",
       "      <th>B1_B1</th>\n",
       "      <th>B2_B1</th>\n",
       "      <th>C1_B1</th>\n",
       "      <th>A2_B2</th>\n",
       "      <th>B1_B2</th>\n",
       "      <th>B2_B2</th>\n",
       "      <th>C1_B2</th>\n",
       "      <th>A2_C1</th>\n",
       "      <th>B1_C1</th>\n",
       "      <th>B2_C1</th>\n",
       "      <th>C1_C1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>Bullet train</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Glass Onion</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Lightyear</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>Matilda(2022)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>The Grinch</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>The Secret Life of Pets.en</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>Thor: love and thunder</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>Up (2009)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.873391</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>5.851931</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "      <td>4.639485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    subs  Level                       Movie  A2_A2  B1_A2  B2_A2  C1_A2  \\\n",
       "237  NaN      2                Bullet train    4.0    4.0    4.0    4.0   \n",
       "235  NaN      3                 Glass Onion    4.0    4.0    4.0    4.0   \n",
       "239  NaN      3                   Lightyear    4.0    4.0    4.0    4.0   \n",
       "236  NaN      4               Matilda(2022)    4.0    4.0    4.0    4.0   \n",
       "240  NaN      2                  The Grinch    4.0    4.0    4.0    4.0   \n",
       "81   NaN      3  The Secret Life of Pets.en    4.0    4.0    4.0    4.0   \n",
       "238  NaN      3      Thor: love and thunder    4.0    4.0    4.0    4.0   \n",
       "106  NaN      1                   Up (2009)    4.0    4.0    4.0    4.0   \n",
       "\n",
       "        A2_B1     B1_B1     B2_B1     C1_B1     A2_B2     B1_B2     B2_B2  \\\n",
       "237  5.873391  5.873391  5.873391  5.873391  5.851931  5.851931  5.851931   \n",
       "235  5.873391  5.873391  5.873391  5.873391  5.851931  5.851931  5.851931   \n",
       "239  5.873391  5.873391  5.873391  5.873391  5.851931  5.851931  5.851931   \n",
       "236  5.873391  5.873391  5.873391  5.873391  5.851931  5.851931  5.851931   \n",
       "240  5.873391  5.873391  5.873391  5.873391  5.851931  5.851931  5.851931   \n",
       "81   5.873391  5.873391  5.873391  5.873391  5.851931  5.851931  5.851931   \n",
       "238  5.873391  5.873391  5.873391  5.873391  5.851931  5.851931  5.851931   \n",
       "106  5.873391  5.873391  5.873391  5.873391  5.851931  5.851931  5.851931   \n",
       "\n",
       "        C1_B2     A2_C1     B1_C1     B2_C1     C1_C1  \n",
       "237  5.851931  4.639485  4.639485  4.639485  4.639485  \n",
       "235  5.851931  4.639485  4.639485  4.639485  4.639485  \n",
       "239  5.851931  4.639485  4.639485  4.639485  4.639485  \n",
       "236  5.851931  4.639485  4.639485  4.639485  4.639485  \n",
       "240  5.851931  4.639485  4.639485  4.639485  4.639485  \n",
       "81   5.851931  4.639485  4.639485  4.639485  4.639485  \n",
       "238  5.851931  4.639485  4.639485  4.639485  4.639485  \n",
       "106  5.851931  4.639485  4.639485  4.639485  4.639485  "
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# найдем пропуски в данных после загрузки\n",
    "new_featch[new_featch['subs'].isna()].sort_values('Movie')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Посчитаем для каждого уровня с вероятностями среднее значение чтобы каждый уровень заполнить своим "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bullet train.srt',\n",
       " 'Glass Onion.srt',\n",
       " 'Lightyear.srt',\n",
       " 'Matilda(2022).srt',\n",
       " 'The Grinch.srt',\n",
       " 'The Secret Life of Pets.en.srt',\n",
       " 'Thor: love and thunder.srt',\n",
       " 'Up (2009).srt']"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выдедем отсоритрованный список фильмов  для которых отсутствую субтитры в папке `'/Datasets/Subtitles'`\n",
    "sorted(set(new_featch['Movie'] + '.srt') - set(films_call))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subs</th>\n",
       "      <th>Level</th>\n",
       "      <th>Movie</th>\n",
       "      <th>A2_A2</th>\n",
       "      <th>B1_A2</th>\n",
       "      <th>B2_A2</th>\n",
       "      <th>C1_A2</th>\n",
       "      <th>A2_B1</th>\n",
       "      <th>B1_B1</th>\n",
       "      <th>B2_B1</th>\n",
       "      <th>C1_B1</th>\n",
       "      <th>A2_B2</th>\n",
       "      <th>B1_B2</th>\n",
       "      <th>B2_B2</th>\n",
       "      <th>C1_B2</th>\n",
       "      <th>A2_C1</th>\n",
       "      <th>B1_C1</th>\n",
       "      <th>B2_C1</th>\n",
       "      <th>C1_C1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [subs, Level, Movie, A2_A2, B1_A2, B2_A2, C1_A2, A2_B1, B1_B1, B2_B1, C1_B1, A2_B2, B1_B2, B2_B2, C1_B2, A2_C1, B1_C1, B2_C1, C1_C1]\n",
       "Index: []"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# удалим фильмы с пропущенными субтитрами\n",
    "new_featch.dropna(inplace=True)\n",
    "# проверим удаление\n",
    "new_featch[new_featch['subs'].isna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Оценим дисбаланс классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Level\n",
       "3    105\n",
       "2     58\n",
       "4     39\n",
       "1     31\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 504,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_featch['Level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Присутсвует дисбаланс в сторону 3 класса. Проводить манипуляции не целесообразно, так как наличие текста подразумевает значительное влиеяние на результат в случае добавления данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Препроцессинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Векторизация текстовых признаков признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(new_featch['subs'])\n",
    "\n",
    "tfidf_df = pd.DataFrame(X.toarray(), index=new_featch['Movie'], columns = vectorizer.get_feature_names_out())\n",
    "all_data = new_featch.merge(tfidf_df, on='Movie', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 31452)"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subs_x</th>\n",
       "      <th>Level</th>\n",
       "      <th>Movie</th>\n",
       "      <th>A2_A2</th>\n",
       "      <th>B1_A2</th>\n",
       "      <th>B2_A2</th>\n",
       "      <th>C1_A2</th>\n",
       "      <th>A2_B1</th>\n",
       "      <th>B1_B1</th>\n",
       "      <th>B2_B1</th>\n",
       "      <th>...</th>\n",
       "      <th>zorbing</th>\n",
       "      <th>zounds</th>\n",
       "      <th>zpd</th>\n",
       "      <th>zsa</th>\n",
       "      <th>zsaz</th>\n",
       "      <th>zuckerberg</th>\n",
       "      <th>zuk</th>\n",
       "      <th>zulu</th>\n",
       "      <th>zurg</th>\n",
       "      <th>zuzu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ben on phone michelle please don t hang up jus...</td>\n",
       "      <td>2</td>\n",
       "      <td>10_Cloverfield_lane(2016)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i ll be right with you so cameron here you go ...</td>\n",
       "      <td>2</td>\n",
       "      <td>10_things_I_hate_about_you(1999)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>should we help him he s due in the lists in tw...</td>\n",
       "      <td>3</td>\n",
       "      <td>A_knights_tale(2001)</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>get to it black eyes open wide it s time to te...</td>\n",
       "      <td>3</td>\n",
       "      <td>A_star_is_born(2018)</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>where the caravan camels roam where it s flat ...</td>\n",
       "      <td>1</td>\n",
       "      <td>Aladdin(1992)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>i assume my deal with edward is dead as long a...</td>\n",
       "      <td>4</td>\n",
       "      <td>Suits.S03E06.720p.HDTV.x264-mSD</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>it s going up on the wall tomorrow and this is...</td>\n",
       "      <td>4</td>\n",
       "      <td>Suits.S03E07.HDTV.x264-mSD</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>darby backs me for managing partner i don t wa...</td>\n",
       "      <td>4</td>\n",
       "      <td>Suits.S03E08.480p.HDTV.x264-mSD</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>i m bonding with your father here speaking of ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Suits.S03E09.480p.HDTV.x264-mSD</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>this is a copy of the letter you wrote a copy ...</td>\n",
       "      <td>4</td>\n",
       "      <td>Suits.S03E10.HDTV.x264-mSD</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>241 rows × 31452 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                subs_x  Level  \\\n",
       "0    ben on phone michelle please don t hang up jus...      2   \n",
       "1    i ll be right with you so cameron here you go ...      2   \n",
       "2    should we help him he s due in the lists in tw...      3   \n",
       "3    get to it black eyes open wide it s time to te...      3   \n",
       "4    where the caravan camels roam where it s flat ...      1   \n",
       "..                                                 ...    ...   \n",
       "236  i assume my deal with edward is dead as long a...      4   \n",
       "237  it s going up on the wall tomorrow and this is...      4   \n",
       "238  darby backs me for managing partner i don t wa...      4   \n",
       "239  i m bonding with your father here speaking of ...      4   \n",
       "240  this is a copy of the letter you wrote a copy ...      4   \n",
       "\n",
       "                                Movie  A2_A2  B1_A2  B2_A2  C1_A2  A2_B1  \\\n",
       "0           10_Cloverfield_lane(2016)    3.0    6.0    4.0    6.0    3.0   \n",
       "1    10_things_I_hate_about_you(1999)    5.0    5.0    5.0    7.0    4.0   \n",
       "2                A_knights_tale(2001)    4.0    4.0    3.0    6.0    3.0   \n",
       "3                A_star_is_born(2018)    3.0    4.0    3.0    7.0    3.0   \n",
       "4                       Aladdin(1992)    5.0    6.0    4.0    6.0    4.0   \n",
       "..                                ...    ...    ...    ...    ...    ...   \n",
       "236   Suits.S03E06.720p.HDTV.x264-mSD    3.0    5.0    5.0    5.0    5.0   \n",
       "237        Suits.S03E07.HDTV.x264-mSD    3.0    5.0    4.0    5.0    4.0   \n",
       "238   Suits.S03E08.480p.HDTV.x264-mSD    2.0    3.0    4.0    4.0    3.0   \n",
       "239   Suits.S03E09.480p.HDTV.x264-mSD    2.0    3.0    4.0    4.0    3.0   \n",
       "240        Suits.S03E10.HDTV.x264-mSD    3.0    6.0    4.0    6.0    3.0   \n",
       "\n",
       "     B1_B1  B2_B1  ...  zorbing  zounds  zpd  zsa  zsaz  zuckerberg  zuk  \\\n",
       "0     10.0    5.0  ...      0.0     0.0  0.0  0.0   0.0         0.0  0.0   \n",
       "1      8.0    5.0  ...      0.0     0.0  0.0  0.0   0.0         0.0  0.0   \n",
       "2      7.0    4.0  ...      0.0     0.0  0.0  0.0   0.0         0.0  0.0   \n",
       "3      8.0    3.0  ...      0.0     0.0  0.0  0.0   0.0         0.0  0.0   \n",
       "4     10.0    6.0  ...      0.0     0.0  0.0  0.0   0.0         0.0  0.0   \n",
       "..     ...    ...  ...      ...     ...  ...  ...   ...         ...  ...   \n",
       "236   11.0    5.0  ...      0.0     0.0  0.0  0.0   0.0         0.0  0.0   \n",
       "237    8.0    5.0  ...      0.0     0.0  0.0  0.0   0.0         0.0  0.0   \n",
       "238    9.0    3.0  ...      0.0     0.0  0.0  0.0   0.0         0.0  0.0   \n",
       "239    7.0    5.0  ...      0.0     0.0  0.0  0.0   0.0         0.0  0.0   \n",
       "240   11.0    6.0  ...      0.0     0.0  0.0  0.0   0.0         0.0  0.0   \n",
       "\n",
       "     zulu  zurg  zuzu  \n",
       "0     0.0   0.0   0.0  \n",
       "1     0.0   0.0   0.0  \n",
       "2     0.0   0.0   0.0  \n",
       "3     0.0   0.0   0.0  \n",
       "4     0.0   0.0   0.0  \n",
       "..    ...   ...   ...  \n",
       "236   0.0   0.0   0.0  \n",
       "237   0.0   0.0   0.0  \n",
       "238   0.0   0.0   0.0  \n",
       "239   0.0   0.0   0.0  \n",
       "240   0.0   0.0   0.0  \n",
       "\n",
       "[241 rows x 31452 columns]"
      ]
     },
     "execution_count": 572,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Разделим данные на выборки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки: ((192, 31449), (192,))\n",
      "Размер тестовой выборки: ((49, 31449), (49,))\n"
     ]
    }
   ],
   "source": [
    "# разобъем данные на валидационную и тестовую выборки\n",
    "data_train, data_test = train_test_split(all_data, random_state=RANDOM_STATE, test_size=.2)\n",
    "# определим независимые признаки и зависимую целевую метку\n",
    "X_train = data_train.drop(['Level', 'Movie', 'subs_x'], axis=1)\n",
    "y_train = data_train['Level']\n",
    "X_test = data_test.drop(['Level', 'Movie', 'subs_x'], axis=1)\n",
    "y_test = data_test['Level']\n",
    "# определим размеры обучающей и тестовой выборки\n",
    "print(f'Размер обучающей выборки: {X_train.shape, y_train.shape}')\n",
    "print(f'Размер тестовой выборки: {X_test.shape, y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Обучим модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (1.2)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: graphviz in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from catboost) (0.20.1)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from catboost) (3.7.1)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from catboost) (1.24.3)\n",
      "Requirement already satisfied: pandas>=0.24 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from catboost) (2.0.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from catboost) (1.10.1)\n",
      "Requirement already satisfied: plotly in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from catboost) (5.15.0)\n",
      "Requirement already satisfied: six in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.24->catboost) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.24->catboost) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from pandas>=0.24->catboost) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->catboost) (1.1.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->catboost) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->catboost) (4.40.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->catboost) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->catboost) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->catboost) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from matplotlib->catboost) (3.0.9)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from plotly->catboost) (8.2.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install catboost\n",
    "from catboost import CatBoostClassifier as cb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 1.3628477\ttotal: 1.63s\tremaining: 1m 19s\n",
      "49:\tlearn: 0.7761546\ttotal: 32.3s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x254dbc239d0>"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = cb(\n",
    "    random_seed=42,\n",
    "    iterations=50,\n",
    "    learning_rate=0.05\n",
    ")\n",
    "       \n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    verbose=50\n",
    ")\n",
    "#pred = model.predict(X_test)\n",
    "#print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00         6\n",
      "           2       0.41      0.64      0.50        11\n",
      "           3       0.63      0.81      0.71        21\n",
      "           4       1.00      0.45      0.62        11\n",
      "\n",
      "    accuracy                           0.59        49\n",
      "   macro avg       0.51      0.48      0.46        49\n",
      "weighted avg       0.59      0.59      0.56        49\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\Asus\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Итак имеем точность предсказаний 0.59. Другими словами уже не ноль(0.5), но и не максимальные горизонты возможного. Можно утверждать, что модель работает, но  совершенству нет предела и возможны доработки в разных направлениях. \n",
    "- Для выпуска модели в продакшн,  наряду с интерфейсом, конечно же потребуется расширение исходной базы данных как фильмов, так и дополнительных списков слов, которые обогатят и сделают более точной модель.\n",
    "- Так как уровни языка это очень субъективная часть жизни и каждый уровень более точен с более широким контекстом. Кроме экспериментов с моделями и гиперпараметрами, входящие данные остаются во главе угла. \n",
    "-  В инжиниринг слудует довносить и структурировать данные с различных сфер. И если говорить о идеальной модели, то базовая версия состоит из условных 25% основной информации, то оставшаяся, не осознанная и по каким то причинам не внесенная, растягивается длинным хвостом не упорядоченная, полезная информация, которая в объеме значительно привышает текущий объем.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
